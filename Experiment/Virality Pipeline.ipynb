{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8b5c80a-9c8f-4f00-b531-ce6cdafa14df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import clip\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image as PILImage\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import IPython\n",
    "from IPython.display import Image, display\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "import math\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed19d10d-a6f0-4511-afca-f668460ce70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.2.3\n",
      "2.32.3\n",
      "10.3.0\n",
      "2.6.0\n",
      "8.32.0\n",
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(requests.__version__)\n",
    "print(PIL.__version__)\n",
    "print(torch.__version__)\n",
    "print(IPython.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8daad077-a678-4a2b-b637-582d0a07ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe9a3bc3-7bdf-4b15-b803-c33bd1774f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "torch.backends.mps.is_available = lambda: False  # Disable MPS explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db244182-e44a-4c36-952e-b1477334099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.device(\"cpu\"))\n",
    "print(torch.cuda.is_available())  # Should be False\n",
    "print(torch.backends.mps.is_available())  # Should be False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad6ddb53-b44e-41c8-8a52-bd412dba2391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "import pickle\n",
    "\n",
    "text_embedding_pipe = pipeline(\"feature-extraction\", model=\"BAAI/bge-small-en-v1.5\", device = \"cpu\")\n",
    "# with open('sentiment_pipeline.pk', 'rb') as f:\n",
    "#     sentiment_pipe = pickle.load(f)\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-sentiment\", device = \"cpu\", truncation = True, max_length = 512)\n",
    "ocr_pipe = easyocr.Reader(['en'], gpu = False)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4c896d2-db19-4601-8b0e-ace30764a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_img_from_url(img_url):\n",
    "    try:\n",
    "        response = requests.get(img_url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch: {img_url} (Status: {response.status_code})\")\n",
    "            return \"\"\n",
    "\n",
    "        # Read image as bytes\n",
    "        img_bytes = response.content\n",
    "        \n",
    "        # Use EasyOCR directly with bytes\n",
    "        result = ocr_pipe.readtext(img_bytes)\n",
    "\n",
    "        # Extract text with high confidence\n",
    "        text_result = \" \".join(text for _, text, prob in result if prob > 0.5)\n",
    "\n",
    "        return text_result if text_result else \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {img_url}: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "def img_embed_from_url(image_url):\n",
    "    # Sample image and text data\n",
    "    response = requests.get(image_url)\n",
    "    # Preprocess image\n",
    "    image = preprocess(\n",
    "        PILImage.open(\n",
    "            BytesIO(response.content)\n",
    "        )\n",
    "    ).unsqueeze(0).to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim = -1, keepdim = True)\n",
    "    return image_features[0]\n",
    "\n",
    "def text_embed_from_str(text):\n",
    "    text = str(text)\n",
    "    embed = text_embedding_pipe(text)[0][0]\n",
    "    return embed\n",
    "\n",
    "def get_sentiment(text):\n",
    "    text = str(text)\n",
    "    res = sentiment_pipe(text)[0]['label']\n",
    "    if res == 'LABEL_2':\n",
    "        return 1\n",
    "    elif res == 'LABEL_1':\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf0e0ace-f445-4e70-92e1-bd740affed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obj (obj):\n",
    "    blank_text_embed = text_embed_from_str('')\n",
    "    blank_img_embed = img_embed_from_url('https://static.vecteezy.com/system/resources/thumbnails/012/680/916/small/blank-black-cement-wall-texture-for-background-with-copy-space-for-design-free-photo.jpg')\n",
    "    \n",
    "    columns_to_normalise = ['totalVolume', 'volume24h', 'marketCap', 'uniqueHolders', 'transferCount']\n",
    "    obj[columns_to_normalise] = scaler.transform(pd.DataFrame([obj[columns_to_normalise]]))[0]\n",
    "    \n",
    "    if obj['name'] is not np.nan:\n",
    "        obj['name_sentiment'] = get_sentiment(obj['name'])\n",
    "        obj['name_embed'] = text_embed_from_str(obj['name'])\n",
    "    elif obj['name'] is np.nan:\n",
    "        obj['name_sentiment'] = 0\n",
    "        obj['name_embed'] = blank_text_embed\n",
    "    \n",
    "    if obj['description'] is not np.nan:\n",
    "        obj['description_sentiment'] = get_sentiment(obj['description'])\n",
    "        obj['description_embed'] = text_embed_from_str(obj['description'])\n",
    "    elif obj['description'] is np.nan:\n",
    "        obj['description_sentiment'] = 0\n",
    "        obj['description_embed'] = blank_text_embed\n",
    "\n",
    "    # IMPORTANT!!!\n",
    "    # Change the name to the corresponding image cdn column\n",
    "    if obj['previewImageUrl'] is not np.nan:\n",
    "        obj['img_embed'] = img_embed_from_url(obj['previewImageUrl'])\n",
    "        obj['img_ocr'] = ocr_img_from_url(obj['previewImageUrl'])\n",
    "    elif obj['previewImageUrl']is np.nan:\n",
    "        obj['img_embed'] = blank_img_embed\n",
    "        obj['img_ocr'] = ''\n",
    "\n",
    "    if obj['img_ocr'] == '':\n",
    "        obj['img_text_embed'] = blank_text_embed\n",
    "        obj['img_text_sentiment'] = 0\n",
    "    \n",
    "    return obj\n",
    "    \n",
    "def preprocess_df (df):\n",
    "    blank_text_embed = text_embed_from_str('')\n",
    "    \n",
    "    blank_img_embed = img_embed_from_url('https://static.vecteezy.com/system/resources/thumbnails/012/680/916/small/blank-black-cement-wall-texture-for-background-with-copy-space-for-design-free-photo.jpg')\n",
    "    \n",
    "    columns_to_normalise = ['totalVolume', 'volume24h', 'marketCap', 'uniqueHolders', 'transferCount']\n",
    "    df[columns_to_normalise] = scaler.fit_transform(df[columns_to_normalise])\n",
    "    \n",
    "    df[['name', 'description']] = df[['name', 'description']].fillna('')\n",
    "\n",
    "    df['name_sentiment'] = df['name'].apply(lambda x: get_sentiment(x) if x != '' else 0 )\n",
    "    df['name_embed'] = df['name'].apply(lambda x: text_embed_from_str(x) if x != '' else blank_text_embed)\n",
    "    \n",
    "    df['description_sentiment'] = df['description'].apply(lambda x: get_sentiment(x) if x != '' else 0 )\n",
    "    df['description_embed'] = df['description'].apply(lambda x: text_embed_from_str(x) if x != '' else blank_text_embed)\n",
    "\n",
    "    df['img_embed'] = df['mediaPreviewUrl'].apply(lambda x: img_embed_from_url(x) if x is not np.nan else blank_img_embed)\n",
    "    df['img_ocr'] = df['mediaPreviewUrl'].apply(lambda x: ocr_img_from_url(x) if x is not np.nan else '')\n",
    "    df['img_text_sentiment'] = df['img_ocr'].apply(lambda x: get_sentiment(x) if x != '' else 0 )\n",
    "    df['img_text_embed'] = df['img_ocr'].apply(lambda x: text_embed_from_str(x) if x != '' else blank_text_embed)\n",
    "\n",
    "    # Convert timestamps to numerical values\n",
    "    df['createdAt'] = pd.to_datetime(df['createdAt'])\n",
    "    T = df['createdAt'].max()  # Latest timestamp\n",
    "    \n",
    "    time_diff = (T - df['createdAt']).dt.total_seconds()\n",
    "    df['time_weight'] = np.exp(-0.001 * (time_diff))\n",
    "\n",
    "    return df\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04425849-4683-4ba4-8156-704f28afcde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x170f7ed23b6cf5d5ac69ba9bd1ea094febfd66ee'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_big = pd.read_csv('Coin.csv')\n",
    "df_coin = pd.read_csv('Coin_new.csv')\n",
    "df_big.address.loc[424]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1d1b26a-e46b-456c-b82f-8891af7ddbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'0x170f7ed23b6cf5d5ac69ba9bd1ea094febfd66ee' in df_coin.address.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fcd6433-cf7e-42bf-8478-990d90bd0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "example_post = df_big.loc[424].copy()\n",
    "example_post_json = example_post.to_dict()\n",
    "with open(\"example_post.json\", \"w\") as file:\n",
    "    json.dump(example_post_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "569a826b-cbcf-4e3d-9615-906e0748be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWVtNmx1eTRwbHNwY3c1YW82bnJ0ZWIyN2t4eTdyazVzYXBnYjVlczQ3eG42d2ltaHZrZ3U=: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error processing https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWd1NTcycno1dXo3dnRpNGJtZDJsdHBzcG5ydGh0NHo2emk3cXd1ZGp1NGZpMmJldWx0emE=: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error processing https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWh4dzZicWdtZWFrZDI3a2Fla3RtdXpxanpwa2plbnZ4amRtNnNkcTJiNjNlYjJtdm5oa3E=: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error processing https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWVqdGkyeG5mbmltZmViaGhneWN6NXdyNW53c29pcjZjZG02cjdyaHhsMjc2em5hd2p2NGE=: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n",
      "Error processing https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWV1N3A2cGtmNW40bHdlZm9tMnBnY2dqbXVidjN5cWc1eW53aWgycGN2N2hhamhoY3F2MnE=: OpenCV(4.11.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:199: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_coin_preprocess = preprocess_df(df_coin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce2f7ceb-c862-45d5-a5b6-d4f7ef1b3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.81 s, sys: 1.2 s, total: 6.01 s\n",
      "Wall time: 2.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "example_post_preprocess = preprocess_obj(example_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7bcdf80-169f-44fd-a1d0-d5e4353599a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'symbol', 'description', 'createdAt', 'creatorAddress',\n",
       "       'uniqueHolders', 'mediaMimeType', 'totalSupply', 'totalVolume',\n",
       "       'volume24h', 'marketCap', 'address', 'mediaPreviewUrl',\n",
       "       'marketCapDelta24h', 'mediaOriginalUri', 'scrapedAt', 'transferCount',\n",
       "       'updatedAt', 'name_sentiment', 'name_embed', 'description_sentiment',\n",
       "       'description_embed', 'img_embed', 'img_ocr', 'img_text_sentiment',\n",
       "       'img_text_embed', 'time_weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coin_preprocess.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "248bb361-f4fd-4273-a13a-a0a24c97dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_calculator(sen1, sen2):\n",
    "    distance = abs(sen1 - sen2)\n",
    "    if distance == 0:\n",
    "        return 1\n",
    "    elif distance == 1:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "def calculate_sim(obj1, obj2):\n",
    "    financial_columns = ['totalVolume', 'volume24h', 'marketCap', 'uniqueHolders', 'transferCount']\n",
    "    w_sentiment = 0.15\n",
    "    w_financial = 0.15\n",
    "    w_embed = 0.1\n",
    "    \n",
    "    sen_ocr = sentiment_calculator(obj1['img_text_sentiment'], obj2['img_text_sentiment']) \n",
    "    sen_name = sentiment_calculator(obj1['name_sentiment'],obj2['name_sentiment'])\n",
    "    sen_description = sentiment_calculator(obj1['description_sentiment'],obj2['description_sentiment']) \n",
    "\n",
    "    img_embed = cosine_similarity([obj1['img_embed']], [obj2['img_embed']])[0][0]\n",
    "    name_embed = cosine_similarity([obj1['name_embed']], [obj2['name_embed']])[0][0]\n",
    "    description_embed = cosine_similarity([obj1['description_embed']], [obj2['description_embed']])[0][0]\n",
    "    img_text_embed = cosine_similarity([obj1['description_embed']], [obj2['description_embed']])[0][0]\n",
    "    \n",
    "    financial_sim = cosine_similarity([obj1[financial_columns]], [obj2[financial_columns]])[0][0]\n",
    "    sentiment_sim = sen_ocr + sen_name + sen_description\n",
    "    embed_sim = img_embed + name_embed + description_embed + img_text_embed\n",
    "    \n",
    "    total_distance = w_sentiment * (sentiment_sim) + w_embed * (embed_sim)  + w_financial * (financial_sim)\n",
    "    return total_distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b70a95dc-f0fc-44f8-8837-51bd79c2a997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8768203870989897"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_sim (example_post_preprocess, df_coin_preprocess.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abdbca63-7305-49f4-ad2f-0488e95cb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coin_preprocess['similarity'] = df_coin_preprocess.apply(lambda x: calculate_sim(example_post_preprocess, x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5447ccbd-952c-4002-9a4d-e2a1ac0a2f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean_similarity = np.average(df_coin_preprocess['similarity'], weights=df_coin_preprocess['time_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd0d8a30-9c60-4150-abe3-d1253ea7371e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6694290906793772"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_mean_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4c5efdd-1c2c-4cec-bd5a-6dc32c8330bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    163.000000\n",
       "mean       0.759401\n",
       "std        0.106842\n",
       "min        0.421587\n",
       "25%        0.702574\n",
       "50%        0.769419\n",
       "75%        0.831577\n",
       "max        0.916695\n",
       "Name: similarity, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coin_preprocess['similarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1cbee9b-20b2-40d4-b91a-0b4beaebfa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('sentiment_pipeline.pk', 'wb') as f:\n",
    "    pickle.dump(sentiment_pipe, f)\n",
    "with open('text_embedding_pipeline.pk', 'wb') as g:\n",
    "    pickle.dump(text_embedding_pipe, g)\n",
    "with open('img_embedding_model.pk', 'wb') as h:\n",
    "    pickle.dump(model, h)\n",
    "with open('img_embedding_preprocess.pk', 'wb') as i:\n",
    "    pickle.dump(preprocess, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd400b-82a5-41db-ba2f-cf3396c1b3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "49bc68be-92f3-4bc9-ba3e-bfc31e121f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a47fca67-f23a-485e-bfbb-709d919768c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/1yj5j5nx6gddj5pqbjbnl45r0000gn/T/ipykernel_78656/1378473980.py:1: DeprecationWarning: The '__version__' attribute is deprecated and will be removed in Flask 3.1. Use feature detection or 'importlib.metadata.version(\"flask\")' instead.\n",
      "  flask.__version__\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.1.0'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flask.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25900021-ef38-452f-b952-4edd58273ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bit about coinsagains'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_img_from_url(\"https://media.decentralized-content.com/-/rs:fit:600:600/f:best/aHR0cHM6Ly9tYWdpYy5kZWNlbnRyYWxpemVkLWNvbnRlbnQuY29tL2lwZnMvYmFmeWJlaWViejdnN2EyeWFhZXZobDIzY3M2dnV2bXczaXFjaWxsZmtlaWNnYnVjZTNuN2FkeDQ2Ym0=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9b0784-8ec5-420b-b11d-8fef7fbe06fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
